{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's download the dataset we are going to study. The [dataset](http://ai.stanford.edu/~amaas/data/sentiment/) has been curated by Andrew Maas et al. and contains a total of 100,000 reviews on IMDB. 25,000 of them are labelled as positive and negative for training, another 25,000 are labelled for testing (in both cases they are highly polarized). The remaning 50,000 is an additional unlabelled data (but we will find a use for it nonetheless).\n",
    "\n",
    "We'll begin with a sample we've prepared for you, so that things run quickly before going over the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/ubuntu/aai/.fastai/data/imdb_sample/data_save.pkl'),\n",
       " PosixPath('/home/ubuntu/aai/.fastai/data/imdb_sample/texts.csv')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It only contains one csv file, let's have a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pathlib.PosixPath"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>is_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>Un-bleeping-believable! Meg Ryan doesn't even ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>This is a extremely well-made film. The acting...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>Every once in a long while a movie will come a...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>Name just says it all. I watched this movie wi...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>This movie succeeds at being one of the most u...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  is_valid\n",
       "0  negative  Un-bleeping-believable! Meg Ryan doesn't even ...     False\n",
       "1  positive  This is a extremely well-made film. The acting...     False\n",
       "2  negative  Every once in a long while a movie will come a...     False\n",
       "3  positive  Name just says it all. I watched this movie wi...     False\n",
       "4  negative  This movie succeeds at being one of the most u...     False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path/'texts.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.<br /><br />But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is some merit in this view, but it\\'s also true that no one forced Hindus and Muslims in the region to mistreat each other as they did around the time of partition. It seems more likely that the British simply saw the tensions between the religions and were clever enough to exploit them to their own ends.<br /><br />The result is that there is much cruelty and inhumanity in the situation and this is very unpleasant to remember and to see on the screen. But it is never painted as a black-and-white case. There is baseness and nobility on both sides, and also the hope for change in the younger generation.<br /><br />There is redemption of a sort, in the end, when Puro has to make a hard choice between a man who has ruined her life, but also truly loved her, and her family which has disowned her, then later come looking for her. But by that point, she has no option that is without great pain for her.<br /><br />This film carries the message that both Muslims and Hindus have their grave faults, and also that both can be dignified and caring people. The reality of partition makes that realisation all the more wrenching, since there can never be real reconciliation across the India/Pakistan border. In that sense, it is similar to \"Mr & Mrs Iyer\".<br /><br />In the end, we were glad to have seen the film, even though the resolution was heartbreaking. If the UK and US could deal with their own histories of racism with this kind of frankness, they would certainly be better off.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It contains one line per review, with the label ('negative' or 'positive'), the text and a flag to determine if it should be part of the validation set or the training set. If we ignore this flag, we can create a DataBunch containing this data in one line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm = TextDataBunch.from_csv(path, 'texts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By executing this line a process was launched that took a bit of time. Let's dig a bit into it. Images could be fed (almost) directly into a model because they're just a big array of pixel values that are floats between 0 and 1. A text is composed of words, and we can't apply mathematical functions to them directly. We first have to convert them to numbers. This is done in two differents steps: tokenization and numericalization. A `TextDataBunch` does all of that behind the scenes for you.\n",
    "\n",
    "Before we delve into the explanations, let's take the time to save the things that were calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next time we launch this notebook, we can skip the cell above that took a bit of time (and that will take a lot more when you get to the full dataset) and load those results like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of processing we make the texts go through is to split the raw sentences into words, or more exactly tokens. The easiest way to do this would be to split the string on spaces, but we can be smarter:\n",
    "\n",
    "- we need to take care of punctuation\n",
    "- some words are contractions of two different words, like isn't or don't\n",
    "- we may need to clean some parts of our texts, if there's HTML code for instance\n",
    "\n",
    "To see what the tokenizer had done behind the scenes, let's have a look at a few texts in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj raising xxmaj victor xxmaj vargas : a xxmaj review \\n \\n  xxmaj you know , xxmaj raising xxmaj victor xxmaj vargas is like sticking your hands into a big , steaming bowl of xxunk . xxmaj it 's warm and gooey , but you 're not sure if it feels right . xxmaj try as i might , no matter how warm and gooey xxmaj raising xxmaj</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxup the xxup shop xxup around xxup the xxup corner is one of the sweetest and most feel - good romantic comedies ever made . xxmaj there 's just no getting around that , and it 's hard to actually put one 's feeling for this film into words . xxmaj it 's not one of those films that tries too hard , nor does it come up with</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj now that xxmaj che(2008 ) has finished its relatively short xxmaj australian cinema run ( extremely limited xxunk screen in xxmaj sydney , after xxunk ) , i can xxunk join both xxunk of \" xxmaj at xxmaj the xxmaj movies \" in taking xxmaj steven xxmaj soderbergh to task . \\n \\n  xxmaj it 's usually satisfying to watch a film director change his style /</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj this film sat on my xxmaj tivo for weeks before i watched it . i dreaded a self - indulgent xxunk flick about relationships gone bad . i was wrong ; this was an xxunk xxunk into the screwed - up xxunk of xxmaj new xxmaj yorkers . \\n \\n  xxmaj the format is the same as xxmaj max xxmaj xxunk ' \" xxmaj la xxmaj ronde</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj many neglect that this is n't just a classic due to the fact that it 's the first xxup 3d game , or even the first xxunk - up . xxmaj it 's also one of the first stealth games , one of the xxunk definitely the first ) truly claustrophobic games , and just a pretty well - xxunk gaming experience in general . xxmaj with graphics</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = TextClasDataBunch.from_csv(path, 'texts.csv')\n",
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The texts are truncated at 100 tokens for more readability. We can see that it did more than just split on space and punctuation symbols: \n",
    "- the \"'s\" are grouped together in one token\n",
    "- the contractions are separated like this: \"did\", \"n't\"\n",
    "- content has been cleaned for any HTML symbol and lower cased\n",
    "- there are several special tokens (all those that begin by xx), to replace unknown tokens (see below) or to introduce different text fields (here we only have one)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have extracted tokens from our texts, we convert to integers by creating a list of all the words used. We only keep the ones that appear at least twice with a maximum vocabulary size of 60,000 (by default) and replace the ones that don't make the cut by the unknown token `UNK`.\n",
    "\n",
    "The correspondance from ids to tokens is stored in the `vocab` attribute of our datasets, in a dictionary called `itos` (for int to string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxunk',\n",
       " 'xxpad',\n",
       " 'xxbos',\n",
       " 'xxeos',\n",
       " 'xxfld',\n",
       " 'xxmaj',\n",
       " 'xxup',\n",
       " 'xxrep',\n",
       " 'xxwrep',\n",
       " 'the']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.vocab.itos[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pepper',\n",
       " 'dell',\n",
       " 'vidor',\n",
       " 'cassandra',\n",
       " 'peterson',\n",
       " 'martino',\n",
       " 'venoms',\n",
       " 'hayao',\n",
       " 'xxfake',\n",
       " 'xxfake']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.vocab.itos[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we look at what a what's in our datasets, we'll see the tokenized text as a representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fastai.data_block.LabelList??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fastai.data_block.LabelList"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data.train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text [  2   5 278  18 ...  15 623  17  11]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_ds[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the underlying data is all numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,    5,  278,   18,  132,  631,   19, 1037,   29,    9])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_ds[0][0].data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With the data block API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the data block API with NLP and have a lot more flexibility than what the default factory methods offer. In the previous example for instance, the data was randomly split between train and validation instead of reading the third column of the csv.\n",
    "\n",
    "With the data block API though, we have to manually call the tokenize and numericalize steps. This allows more flexibility, and if you're not using the defaults from fastai, the various arguments to pass will appear in the step they're revelant, so it'll be more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = (TextList.from_csv(path, 'texts.csv', cols='text')\n",
    "                .split_from_df(col=2)\n",
    "                .label_from_df(cols=0)\n",
    "                .databunch())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that language models can use a lot of GPU, so you may need to decrease batchsize here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's grab the full dataset for what follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/ubuntu/aai/.fastai/data/imdb/imdb.vocab'),\n",
       " PosixPath('/home/ubuntu/aai/.fastai/data/imdb/data_clas.pkl'),\n",
       " PosixPath('/home/ubuntu/aai/.fastai/data/imdb/ld.pkl'),\n",
       " PosixPath('/home/ubuntu/aai/.fastai/data/imdb/README'),\n",
       " PosixPath('/home/ubuntu/aai/.fastai/data/imdb/unsup'),\n",
       " PosixPath('/home/ubuntu/aai/.fastai/data/imdb/train'),\n",
       " PosixPath('/home/ubuntu/aai/.fastai/data/imdb/tmp_lm'),\n",
       " PosixPath('/home/ubuntu/aai/.fastai/data/imdb/models'),\n",
       " PosixPath('/home/ubuntu/aai/.fastai/data/imdb/ll_clas.pkl'),\n",
       " PosixPath('/home/ubuntu/aai/.fastai/data/imdb/data_lm.pkl'),\n",
       " PosixPath('/home/ubuntu/aai/.fastai/data/imdb/test'),\n",
       " PosixPath('/home/ubuntu/aai/.fastai/data/imdb/tmp_clas')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.IMDB)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/ubuntu/aai/.fastai/data/imdb/train/neg'),\n",
       " PosixPath('/home/ubuntu/aai/.fastai/data/imdb/train/pos'),\n",
       " PosixPath('/home/ubuntu/aai/.fastai/data/imdb/train/labeledBow.feat'),\n",
       " PosixPath('/home/ubuntu/aai/.fastai/data/imdb/train/unsupBow.feat')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/'train').ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reviews are in a training and test set following an imagenet structure. The only difference is that there is an `unsup` folder on top of `train` and `test` that contains the unlabelled data.\n",
    "\n",
    "We're not going to train a model that classifies the reviews from scratch. Like in computer vision, we'll use a model pretrained on a bigger dataset (a cleaned subset of wikipedia called [wikitext-103](https://einstein.ai/research/blog/the-wikitext-long-term-dependency-language-modeling-dataset)). That model has been trained to guess what the next word is, its input being all the previous words. It has a recurrent structure and a hidden state that is updated each time it sees a new word. This hidden state thus contains information about the sentence up to that point.\n",
    "\n",
    "We are going to use that 'knowledge' of the English language to build our classifier, but first, like for computer vision, we need to fine-tune the pretrained model to our particular dataset. Because the English of the reviews left by people on IMDB isn't the same as the English of wikipedia, we'll need to adjust the parameters of our model by a little bit. Plus there might be some words that would be extremely common in the reviews dataset but would be barely present in wikipedia, and therefore might not be part of the vocabulary the model was trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the unlabelled data is going to be useful to us, as we can use it to fine-tune our model. Let's create our data object with the data block API (next line takes a few minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm = (TextList.from_folder(path)\n",
    "           #Inputs: all the text files in path\n",
    "            .filter_by_folder(include=['train', 'test', 'unsup']) \n",
    "           #We may have other temp folders that contain text files so we only keep what's in train and test\n",
    "            .split_by_rand_pct(0.1)\n",
    "           #We randomly split and keep 10% (10,000 reviews) for validation\n",
    "            .label_for_lm()           \n",
    "           #We want to do a language model so we label accordingly\n",
    "            .databunch(bs=bs))\n",
    "data_lm.save('data_lm.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to use a special kind of `TextDataBunch` for the language model, that ignores the labels (that's why we put 0 everywhere), will shuffle the texts at each epoch before concatenating them all together (only for training, we don't shuffle for the validation set) and will send batches that read that text in order with targets that are the next word in the sentence.\n",
    "\n",
    "The line before being a bit long, we want to load quickly the final ids by using the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = load_data(path, 'data_lm.pkl', bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>. xxmaj it 's obvious from the outset that this is going to be an entirely bizarre film as the film opens up with a scene set on the moon . xxmaj things do n't get any clearer after that as the lunar sequence turns out to be the dream of xxmaj alice , a troubled woman . xxmaj alice is tormented by dreams of an astronaut stranded on the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>a pleasure to see xxmaj audrey xxmaj tatou . \\n \\n  xxmaj however , i have a very strong issue with part of the movie . xxmaj the lesbian roommate tells xxmaj xavier that women like to be physically dominated ( which i take issue with ) and shows him some sort of butt - grabbing move that 's guaranteed to get a woman . xxmaj xavier then tries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>made , but xxmaj miss xxmaj parker 's performance is , in this cheesy - movie - lover 's opinion , among the worst performances of all time . xxmaj her last scene is especially overplayed . i loved it ! xxbos everyone is a genius in something . xxmaj albert xxmaj einstien was a genius in science , xxmaj william xxmaj shakespeare was a genius in literature and the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>foster . \" xxmaj the short - lived xxmaj girdler co - wrote this thoroughly routine private eye potboiler with producer xxmaj david xxmaj shelton in one night and it features a headstrong female shamus that refuses to rely on a man to help her take care of business . xxmaj unfortunately , \" xxmaj sheba xxmaj baby \" is n't nearly as good as the blaxploitation movies that xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>, and the final scenes of the bugs scrambling for their lives upon a rising skyscraper is some of the best staging and animation of any animated film past and present . \\n \\n  xxmaj do not miss this wonderfully hand drawn film . xxmaj also do n't fail to appreciate the title sequence with the most elaborate example of xxmaj max xxmaj fleischer 's remarkable xxup 3-d xxunk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then put this in a learner object very easily with a model loaded with the pretrained weights. They'll be downloaded the first time you'll execute the following line and stored in `~/.fastai/models/` (or elsewhere if you specified different paths in your config file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.82 s, sys: 348 ms, total: 2.16 s\n",
      "Wall time: 2.05 s\n"
     ]
    }
   ],
   "source": [
    "%time learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='99' class='' max='8054' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      1.23% [99/8054 00:58<1:18:48 11.5509]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n",
      "CPU times: user 46.4 s, sys: 13.4 s, total: 59.8 s\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%time learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot(skip_end=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.188942</td>\n",
       "      <td>4.019735</td>\n",
       "      <td>0.295705</td>\n",
       "      <td>1:23:41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 1e-2, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('fit_head')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('fit_head');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete the fine-tuning, we can then unfeeze and launch a new training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='6' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      60.00% [6/10 9:26:20<6:17:33]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.858057</td>\n",
       "      <td>3.811999</td>\n",
       "      <td>0.317821</td>\n",
       "      <td>1:33:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.814346</td>\n",
       "      <td>3.769908</td>\n",
       "      <td>0.325217</td>\n",
       "      <td>1:33:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.770940</td>\n",
       "      <td>3.742404</td>\n",
       "      <td>0.329003</td>\n",
       "      <td>1:33:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.721550</td>\n",
       "      <td>3.705512</td>\n",
       "      <td>0.333824</td>\n",
       "      <td>1:34:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.646548</td>\n",
       "      <td>3.675458</td>\n",
       "      <td>0.337036</td>\n",
       "      <td>1:35:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.592173</td>\n",
       "      <td>3.648758</td>\n",
       "      <td>0.340161</td>\n",
       "      <td>1:36:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='6187' class='' max='8054' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      76.82% [6187/8054 1:11:02<21:26 3.5118]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(10, 1e-3, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('fine_tuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is our model? Well let's try to see what it predicts after a few given words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('fine_tuned');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = \"I liked this movie because\"\n",
    "N_WORDS = 40\n",
    "N_SENTENCES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I liked this movie because it was so different from the another Bollywood movie . It was based on a true story . The movie works because of the way the actors play their characters . They are very real and\n",
      "I liked this movie because it was a lot better than i had expected . i liked the action sequences and the whole movie was very high . The acting was good from all the actors and the story line was a bit predictable\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to save not only the model, but also its encoder, the part that's responsible for creating and updating the hidden state. For the next part, we don't care about the part that tries to guess the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('fine_tuned_enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create a new data object that only grabs the labelled data and keeps those labels. Again, this line takes a bit of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_clas = (TextList.from_folder(path, vocab=data_lm.vocab)\n",
    "             #grab all the text files in path\n",
    "             .split_by_folder(valid='test')\n",
    "             #split by train and valid folder (that only keeps 'train' and 'test' so no need to filter)\n",
    "             .label_from_folder(classes=['neg', 'pos'])\n",
    "             #label them all with their folders\n",
    "             .databunch(bs=bs))\n",
    "\n",
    "data_clas.save('data_clas.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = load_data(path, 'data_clas.pkl', bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj this movie was recently released on xxup dvd in the xxup us and i finally got the chance to see this hard - to - find gem . xxmaj it even came with original theatrical previews of other xxmaj italian horror classics like \" xxup xxunk \" and \" xxup beyond xxup the xxup darkness \" . xxmaj unfortunately , the previews were the best thing about this</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxup the xxup shop xxup around xxup the xxup corner is one of the sweetest and most feel - good romantic comedies ever made . xxmaj there 's just no getting around that , and it 's hard to actually put one 's feeling for this film into words . xxmaj it 's not one of those films that tries too hard , nor does it come up with</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos * * * xxup spoilers * * * * * * xxup spoilers * * * xxmaj continued ... \\n \\n  xxmaj from here on in the whole movie collapses in on itself . xxmaj first we meet a rogue program with the indication we 're gon na get ghosts and vampires and werewolves and the like . xxmaj we get a guy with a retarded accent talking</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj prior to this release , xxmaj neil labute had this to say about the 1973 original : \" xxmaj it 's surprising how many people say it 's their favorite soundtrack . i 'm like , come on ! xxmaj you may not like the new one , but if that 's your favorite soundtrack , i do n't know if i * want * you to like</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_clas.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create a model to classify those reviews and load the encoder we saved before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (25000 items)\n",
       "x: TextList\n",
       "xxbos i like to keep my reviews short and simple , but this pretty much sums it up . xxmaj you can not beat the original two for a number of reasons one of which including the directing talent of xxmaj chris xxmaj xxunk . \n",
       " \n",
       "  xxmaj this movie had terrible directing covered up by even more terrible acting . i ca nt even believe these people are considered actors . \n",
       " \n",
       "  xxmaj painful to sit through and watch . xxmaj the storyline was a complete joke about a secret chip and xxmaj russian terrorists on a painstaking quest to get it back . xxmaj horrible , rent one of the original tow and enjoy yourself ! \n",
       " \n",
       "  xxmaj the movie was n't even set during xxmaj christmas like the original . xxmaj home xxmaj alone was turned from an excellent xxmaj christmas time family comedy movie to a joke with no moral or plot !,xxbos xxmaj why was this film made ? xxmaj even keeping in mind the generous tax concessions that xxmaj australian film investors were given , there can be no reasonable explanation for this film being given the go - ahead . xxmaj for goodness sakes , the actors cast in this film are xxmaj aussie b - grade celebs ( not actors , people like xxmaj john xxmaj michael ' xxmaj hollywood ' xxmaj xxunk , the original drummer from the band in xxmaj hey xxmaj hey xxmaj its xxmaj saturday , and the voice - over guy in xxmaj countdown . xxmaj but in saying that , this is still very watchable as long as you give it the brain attention it deserves : none . xxmaj the script is bad ( even for a self - confessed b - grade horror ) and the acting and film quality is worse . xxmaj it often looks as though it is a home movie , but even a home movie has ' realism ' . xxmaj anyone interested in xxmaj australian cinema , please , for the love of xxmaj god , pretend this film was xxup never made .,xxbos i really looked forward to this program for two reasons ; i really liked xxmaj jan xxmaj michael xxmaj vincent and i am an aviation nut and have a serious love affair with helicopters . i do n't like this program because it takes fantasy to an unbelievable level . xxmaj the world speed record for helicopters was set at xxunk mph by a xxmaj xxunk xxmaj lynx several years ago . xxmaj the only chopper that was ever faster was the experimental xxmaj xxunk xxup xxunk in the 1960 's . xxmaj it hit over 300 and was a compound helicopter , which means it had a pusher propeller at the end of its fuselage providing thrust . \n",
       " \n",
       "  xxmaj in short , no helicopter can fly much over 275 because of the principle of rotary wing flight . xxmaj and the xxmaj bell 222 , the \" actor \" that portrayed xxmaj airwolf was n't very fast even by helicopter standards . xxmaj and it did n't stay in production very long . \n",
       " \n",
       "  xxmaj there was a movie that came out during this time period called \" xxmaj blue xxmaj thunder \" that was much more realistic .,xxbos xxmaj the \" saucy \" misadventures of four au pairs who arrive in xxmaj london on the same day in the early 1970s . xxmaj there 's a xxmaj swedish girl , a xxmaj danish , a xxmaj german and a xxmaj chinese . xxmaj the story contrives to get the clothes off all of them , involve them in some xxmaj carry xxmaj on - type humour and couple them with various misfits from the xxmaj british film and xxup tv culture of the time , including xxmaj man xxmaj about the xxmaj house star xxmaj richard o'sullivan , future xxmaj coronation xxmaj street rogue xxmaj johnny xxmaj briggs and horror film stalwart xxmaj ferdy xxmaj mayne ( playing a sheik ) . xxmaj there 's a pretty risqué amount of female nudity on display , for those who like that kind of thing ( but obviously nothing hardcore ) . \n",
       " \n",
       "  xxmaj most of the film is pretty thin and inconsequential ; the girls are stereotypes , and xxmaj german xxmaj anita especially suffers from some kind of xxunk disorder - she 's a moron obsessed with colour xxup tv who acts like a kind of uninhibited child & dresses to deliberately show her private parts ; in another more serious film , she would be a psychiatric case . xxmaj the most interesting section of the film involves the xxmaj swedish girl being taken to a club in xxmaj london where some dodgy types are still trying to swing , being seduced by a middle - aged rocker , losing her virginity and realising that the scene is not for her . xxmaj these sequences have some energy in them and point to a more intriguing film than we 've ended up with , in which promiscuity and the dregs of the music business and upper classes live soulless and seedy lives ( there 's a fine turn by xxmaj john xxmaj standing as an impotent public school xxunk ) . xxmaj the strangest of the stories has the xxmaj chinese girl ( future cannibal film veteran xxmaj me xxmaj me xxmaj lay ) getting off with her childish piano prodigy employer , falling mutually in love with and then leaving in the middle of the night for no good reason at all , except some orientalist notion that \" xxmaj chinese birds are inscrutable , ai n't they ? ! \" xxmaj the film is pretty demeaning to its women characters and there 's a smattering of homophobia in the dialogue and one of the characterisations . xxmaj the end is striking , as xxmaj mayne 's sheik for no earthly reason ( except they have to end the film somehow ) whisks all of the girls away to his xxmaj arab kingdom for what looks to all the world like a future in the white slave trade , which they are all delighted about . \n",
       " \n",
       "  xxmaj stuff and nonsense for the most part then , but directed with a fair amount of skill by veteran xxmaj val xxmaj guest , which puts it as a piece of film - making a notch above most of the 70s xxmaj brit sexploitation flicks .,xxbos xxmaj the 1998 xxmaj michael xxmaj keaton kiddie comedy of the same title was roundly condemned for it 's , um , shoddy special effects , but compared to what xxmaj screaming xxmaj mad xxmaj george cooked up for this horror comedy they 're positively mind - boggling . xxmaj the killer snowman seems to be made out of styrofoam and his arms look like oversized oven mitts . xxmaj which they probably were . xxmaj the cast lays it on thick in this parody of dozens of other ( much worse ) movies and xxmaj paul xxmaj keith as the town doctor is particularly memorable in a small but hilarious role .\n",
       "y: CategoryList\n",
       "neg,neg,neg,neg,neg\n",
       "Path: /home/ubuntu/aai/.fastai/data/imdb;\n",
       "\n",
       "Valid: LabelList (25000 items)\n",
       "x: TextList\n",
       "xxbos xxmaj all i can say about this movie , is it is absolutely boring . xxmaj the intro to the movie is quite possibly the worst intro to a horror film i have ever seen , i mean a angry chick hitting a guy in the head with a frying pan is n't at all frightening which is what i assume the director was aiming for , but in fact it was \" mildly \" funny . \n",
       " \n",
       "  xxmaj the acting in this picture was beyond pathetic ; a note to directors , if your making a horror film , please hire some good actors , not some popular teen soap star who has no idea how to act . \n",
       " \n",
       "  xxmaj the death scenes in this movie were beyond boring ... no gore , and i 'm sorry but horror movies without gore , or good suspense are just cheesy . i mean this girl gets killed by hair wrapped all around her in the middle of xxmaj tokyo , and not one person sees it happen , they just declare her as \" missing \" , wow that s awesome ! \n",
       " \n",
       "  xxmaj in conclusion if you and your friends want to see this movie , make sure u bring some sleeping pills , because i guarantee you wo n't make it to the end .. xxmaj me and my friend walked out cause we did n't even care what happened at the end . \n",
       " \n",
       "  xxmaj cheers,xxbos xxmaj please do not go see this . i did have several laughs throughout this movie , but they were all due to unintentional comedy . \n",
       " \n",
       "  xxmaj there were only three characters in this movie , so it was amazing how bad the character development was . xxmaj pacino played xxmaj pacino again and was aggravating most of the time . xxmaj the scenes in this movie seem like they were put together from 20 other bad movies by a really poor editor . xxmaj there is no continuity and i found myself wondering why i did n't leave 15 minutes into this . \n",
       " \n",
       "  i would suggest never seeing a movie directed by xxup d.j. xxmaj caruso . xxmaj this really was awful .,xxbos xxmaj based on the xxmaj elmore xxmaj leonard novel of the same name , xxmaj killshot suffers from a lack of focus , direction , and creativity  all elements which the original story likely had , and negative test screenings forced severe edits , ( including the complete xxunk of a character ) resulting in a film that feels almost nothing like a xxmaj leonard story . xxmaj far too many characters populate a storyline too simplistic and straightforward ( not a typical trait of the author 's work ) and the focus continually switches between two hit men who are difficult to like and a troubled couple who do n't command our sympathy . xxmaj while the story itself provides precious few twists and turns , sadly by the end of the film its appeal still remains a mystery . \n",
       " \n",
       "  xxmaj washed - up hit - man xxmaj armand \" xxmaj the xxmaj blackbird \" xxmaj degas ( xxmaj mickey xxmaj rourke ) follows a strict code during his missions that inadvertently xxunk his latest assignment . xxmaj now on the run from his former employer , he haphazardly joins forces with inept misfit criminal xxmaj richie xxmaj nix ( xxmaj joseph xxmaj gordon - xxmaj levitt ) to gain some quick cash by extorting a wealthy realtor . xxmaj when struggling couple xxmaj carmen and xxmaj wayne xxmaj xxunk ( xxmaj diane xxmaj lane and xxmaj thomas xxmaj jane ) are privy to the thieves ' xxunk plot , they are forced into hiding as the crazed killers will stop at nothing to silence the two witnesses . \n",
       " \n",
       "  xxmaj killshot proves that being based on an xxmaj elmore xxmaj leonard novel is n't grounds for immediate success or even a promising adaptation . xxmaj the characters , situations , and even resolutions in the film are all tired and unoriginal and only very randomly hint at something more . xxmaj it 's not that there was n't potential , especially when xxmaj rourke 's black - garbed , calm and collected assassin perfectly executes a hit during the opening scene  purpose and principals are just continually abandoned as each minute ticks away . xxmaj the style and manner in which each character is introduced is the most intriguing ; visually the roles of xxmaj bird and even xxmaj wayne are fleshed out xxunk , giving immediate interest and depth to personas that typically end in a creative impasse . \n",
       " \n",
       "  xxmaj the pairing of the cold and calculating xxmaj black xxmaj bird with the irrational and explosive xxmaj richie is an enticing combination ( comparisons to xxmaj fargo would be extravagantly too kind ) , except that each character seems to slowly lose track of the traits that kept them initially interesting . xxmaj as xxmaj richie starts picking up the more experienced killer 's habits , xxmaj bird loosens his grip on his own methods of murder . xxmaj regardless of what he sees in his momentary lighthearted fling with xxmaj donna ( xxmaj rosario xxmaj dawson ) , it 's hard to imagine that his final confrontation with panicky xxmaj carmen would provoke a confession of his true nature and subsequent carelessness that drastically affects his outcome . xxmaj likely or not , this is xxmaj killshot 's unfortunate downfall  and little entertainment can be garnered from these characters who steadily lose their originality by continually contradicting the habits that once made them intriguing . \n",
       " \n",
       "  - xxmaj the xxmaj massie xxmaj twins,xxbos i saw the first xxmaj house of the xxmaj dead and expected a root canal to be more pleasant to attend , so when it was n't as bad as that , i was delightfully surprised . \n",
       " \n",
       "  xxmaj unfortunately , i then got my hopes up that the second one might be okay as well ... and i was wrong . \n",
       " \n",
       "  xxmaj apparently i 'm one of the few people who saw this movie that thinks it was bad . \n",
       " \n",
       "  i do n't know whether to watch it again and force myself to see whatever all the people who gave it good reviews saw , or wonder if i saw the wrong movie . \n",
       " \n",
       "  xxmaj ed xxmaj quinn as xxmaj ellis and xxmaj emmanuelle xxmaj vaugier as xxmaj alexandra ' xxmaj nightingale ' xxmaj morgan did a great job in roles that were way beneath them . xxmaj they deserve to be in better movies . \n",
       " \n",
       "  xxmaj the special effects were okay and some of the characters likable / hate - able and that made for a tolerable watch , but for the most part , this movie was just a waste of time . \n",
       " \n",
       "  xxmaj oh and i have to ask this because i found myself asking it aloud xxup all the way through the movie ... did anyone not know how to close doors behind themselves so zombies would n't just wander into the rooms ? xxmaj only once did it happen , ( zombies wandering in ) and i found that a little convenient ... soldiers walk into a room , leave the door wide open , pay little to no attention to same said door so the zombies can just walk in if they feel like it ( with the hapless \" livings \" being cornered with no way to escape ) and yet only once did zombies follow them in . \n",
       " \n",
       "  xxmaj nitpicky ? xxmaj maybe but honestly ... if i was fighting for my life , the last thing i 'd do would be to walk into a room and leave the door wide open so zombies could swarm in and eat me . \n",
       " \n",
       "  xxmaj that is really the only thing * bothered * me throughout the movie , and just the movie for the most part was a bad sequel to a not totally abominable original .,xxbos xxmaj this quirky and watchable film is the story of a deluded dentist who starts out on his mission or crusade to fight tooth decay in the back and xxunk of xxmaj patagonia . xxmaj hailing from xxmaj northern xxmaj ireland , via xxmaj new xxmaj jersey , the main character , xxmaj fergus , sees his crusade as a mission of mass importance and approaches it with all the enthusiasm , vitality , discipline and attention to detail one would expect from a trained dentist . xxmaj which adds to the hilarity , as his grand plans unravel and gradually fall to pieces as he goes from disaster to debacle in the xxmaj xxunk outback on the back of a customised motor bike or his , er , mobile dental unit . xxmaj we never get to meet his wife , nor the rich philanthropist who is sponsoring the ill - fated mission , but , we do get a solid display from xxmaj lewis . xxmaj fans of his work will not be disappointed with his very believable performance as the deluded dentist who is gallantly adored by the , innocent but sexy , 18 year old female lead who tags along on for the * ahem * ride . \n",
       " \n",
       "  xxmaj this film is not for everyone and i can understand why it was n't pushed by the suits . xxmaj it 's a low budget , sometimes charming , always disarming , mildly amusing and instantly forgettable film that sets out with low expectations and almost succeeds .\n",
       "y: CategoryList\n",
       "neg,neg,neg,neg,neg\n",
       "Path: /home/ubuntu/aai/.fastai/data/imdb;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(60000, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(60000, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f4500280a60>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/home/ubuntu/aai/.fastai/data/imdb'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: ...\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): Embedding(60000, 400, padding_idx=1)\n",
       "  (1): EmbeddingDropout(\n",
       "    (emb): Embedding(60000, 400, padding_idx=1)\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(400, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 1152, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): WeightDropout(\n",
       "    (module): LSTM(1152, 400, batch_first=True)\n",
       "  )\n",
       "  (1): RNNDropout()\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=1200, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5)\n",
    "learn.load_encoder('fine_tuned_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, 2e-2, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('first');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('second');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('third')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 92.00 MiB (GPU 0; 11.17 GiB total capacity; 10.54 GiB already allocated; 11.31 MiB free; 10.88 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-a5d1f8d339a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'third'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, file, device, strict, with_opt, purge, remove_module)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34mf'{file}.pth'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_pathlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mdistrib_barrier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'opt'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0mmodel_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    727\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_torch_load_uninitialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mview_metadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mrestore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mstorage_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mstorage_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_new\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;31m# We may need to call lazy init again if we are a forked child\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;31m# del _CudaBase.__new__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 0; 11.17 GiB total capacity; 10.54 GiB already allocated; 11.31 MiB free; 10.88 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "learn.load('third');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/2 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='419' class='' max='520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      80.58% [419/520 11:19<02:43 0.1644]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 42.00 MiB (GPU 0; 11.17 GiB total capacity; 10.58 GiB already allocated; 11.31 MiB free; 10.88 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-7b4d0a1b41b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I really loved that movie, it was awesome!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, item, return_x, batch_first, with_dropout, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;34m\"Return predicted class, label and probabilities for `item`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_dropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m         \u001b[0mraw_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrab_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'norm'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mpred_batch\u001b[0;34m(self, ds_type, batch, reconstruct, with_dropout, activ)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mactiv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mifnone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactiv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_loss_func2activ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwith_dropout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_dropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_loss_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/fastai/text/learner.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mraw_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbptt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbptt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                 \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbptt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/fastai/text/models/awd_lstm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, from_embeddings)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mnew_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mraw_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhid_dp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m             \u001b[0mraw_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0mnew_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m             \u001b[0mraw_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/fastai/text/models/awd_lstm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;31m#To avoid the warning that comes because the weights aren't flattened.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    570\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    571\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 0; 11.17 GiB total capacity; 10.58 GiB already allocated; 11.31 MiB free; 10.88 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "learn.predict(\"I really loved that movie, it was awesome!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
